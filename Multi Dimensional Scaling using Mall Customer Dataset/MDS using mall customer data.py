# -*- coding: utf-8 -*-
"""2348409_LAB06.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w_ueBpEqRs09kbVrVFt9TddjDeYdYF8n

**Lab 06 - Multi-Dimensional Scaling**

* *Created by: Angeline A*
* *Reg No: 2348409*
* Edited on: 01/04/2024
* Submitted on: 01/04/2024

The code snippet imports essential libraries and modules for data manipulation, visualization, and dimensionality reduction using MDS, along with a utility function for calculating pairwise distances. It sets up the environment for further data analysis and visualization tasks.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.manifold import MDS
from sklearn.metrics import pairwise_distances

"""The below code is reading the dataset and storing the dataset in the variable data."""

# Load the Breast Cancer Wisconsin (Diagnostic) Data Set
data = pd.read_csv("/content/Mall_Customers.csv")
data

"""The below code generates a violin plot to visualize the distribution of annual income among different genders in the dataset, making it easy to compare income distributions across gender categories."""

# @title Gender vs Annual Income (k$)

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(data['Gender'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(data, x='Annual Income (k$)', y='Gender', inner='stick', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

"""From the above graph, we can infer that
* The data shows the annual income distribution between genders, possibly male and female.
* The x-axis represents gender, and the y-axis represents annual income in thousands of dollars (k$).
* The wider part of the violin shows the spread of the data between the first and third quartiles, and the narrower part (the “waist”) represents the interquartile range (IQR), which is the range that contains the middle 50% of the data.
* The data points outside the violin plot represent outliers.

* Based on the violin plot, it appears that the distribution of annual income is wider for males than females, and there may be more outliers in the male data set.

The below code generates a scatter plot to visualize the relationship between annual income and spending score in the dataset, helping to understand how these two variables are related and whether there are any patterns or clusters present.
"""

# @title Annual Income (k$) vs Spending Score (1-100)

from matplotlib import pyplot as plt
data.plot(kind='scatter', x='Annual Income (k$)', y='Spending Score (1-100)', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

"""In this scatter plot,
* The x-axis represents annual income in thousands of dollars (k$) and the y-axis represents spending score (1-100).

* There is a positive correlation between annual income and spending score in the scatter plot.
* This means that as annual income increases, spending score also tends to increase.
* However, the correlation is not perfect, and there is a significant amount of scatter in the data points.
* This indicates that there are other factors that influence spending score besides annual income.

* Here are some other things that you might be able to learn from the scatter plot:
 * The range of spending scores for people with different incomes.
Whether there are any outliers in the data.
 * Whether the relationship between annual income and spending score is linear or non-linear.
 * It is important to note that correlation does not necessarily equal causation.
 * Just because there is a positive correlation between annual income and spending score does not mean that having a higher income causes people to spend more money.
 * There could be other factors that explain the relationship, such as people with higher incomes having more expensive tastes or living in areas with a higher cost of living.

The below code generates a line plot to visualize the relationship between annual income and age in the dataset, grouped by gender, providing insights into how these variables vary with each other.
"""

# @title Annual Income (k$) vs Age

from matplotlib import pyplot as plt
import seaborn as sns
def _plot_series(series, series_name, series_index=0):
  palette = list(sns.palettes.mpl_palette('Dark2'))
  xs = series['Annual Income (k$)']
  ys = series['Age']

  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = data.sort_values('Annual Income (k$)', ascending=True)
for i, (series_name, series) in enumerate(df_sorted.groupby('Gender')):
  _plot_series(series, series_name, i)
  fig.legend(title='Gender', bbox_to_anchor=(1, 1), loc='upper left')
sns.despine(fig=fig, ax=ax)
plt.xlabel('Annual Income (k$)')
_ = plt.ylabel('Age')

"""According to the graph,
* The average income for both men and women increases with age.  From the ages of 20 to 60, the average income for women is higher than the average income for men.
* After the age of 60, the data points for the graph are not shown, so it is impossible to say what happens to the average income for men and women after that age.

The below code generates a pairplot to visualize the relationships between different variables in the dataset, with each data point colored based on its corresponding 'CustomerID'.
"""

import seaborn as sns

sns.pairplot(data, hue='CustomerID')
plt.title('Pairplot of Mall Customer Segmentation Data')
plt.show()

"""* Each subplot shows the distribution of two variables for the same set of data points.
* The diagonal plots along the main diagonal are histograms, which show the distribution of a single variable.

* In this specific case, the data points represent mall customers and the variables include:

  * Age
  * Annual Income (in thousands of dollars)
  * Spending Score (points between 1 and 100)
* The scatter plots show how these variables relate to each other. For example, the top left scatter plot shows the relationship between age and annual income. It appears there might be a weak positive correlation, where customers with higher incomes tend to be older. However, there is significant spread in the data, indicating many younger customers also have high incomes.

* Similarly, the other scatter plots show how spending score relates to age and annual income. There appears to be a weak positive correlation between spending score and income, and no clear correlation between spending score and age.

* The below code generates a boxplot to visualize the distribution of annual income among different genders.
* The boxplot provides information about the median, quartiles, and potential outliers in the data for each gender category.
"""

plt.figure(figsize=(10, 6))
sns.boxplot(x='Gender', y='Annual Income (k$)', data=data)
plt.title('Boxplot of Annual Income by Gender')
plt.show()

"""Based on the boxplot,
* It appears that there is a significant difference in the amount of annual income between men and women.
* The median income for men is higher than the median income for women.
* The boxplot also shows that the distribution of annual income is wider for males than females, and there may be more outliers in the male data set.
* The boxplot suggests that there is a gender pay gap, but it does not tell the whole story.
* More analysis would be needed to understand the reasons for the difference in income.

The below code generates a countplot to visualize the distribution of ages among different genders. The countplot shows the frequency of each age value in the dataset, differentiated by gender.
"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
ax = sns.countplot(x='Age', data=data, hue='Gender')
plt.title('Countplot of Age Distribution by Gender')
plt.xticks(rotation=90)  # Rotate x-axis labels by 90 degrees
plt.show()

"""From the above graph, we can notice how many people are there in both the genders accroding to their age.

The below code generates a heatmap to visualize the correlation between different variables in the dataset. The heatmap helps identify patterns and relationships between variables, with correlation coefficients ranging from -1 to 1.
"""

plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Mall Customer Segmentation Data')
plt.show()

"""* In this specific heatmap, the features or variables include:

  * CustomerID (likely an identifier for each customer)
  * Age
  * Annual Income (in thousands of dollars)
  * Spending Score (between 1 and 100)
* The color intensity in the heatmap represents the correlation coefficient between each pair of variables.  
* A value of 1 indicates a perfect positive correlation, 0 indicates no correlation, and -1 indicates a perfect negative correlation.

* Here are some of the insights you can get from the heatmap:

 * Age and Annual Income:
   * There is a weak positive correlation between age and annual income (around 0.03).
   * This means that as a customer's age increases, their annual income also tends to increase, but the correlation is weak.
 * Age and Spending Score:
   * There is a very weak negative correlation between age and spending score (around -0.03).
   * This means that there might be a slight tendency for younger customers to have higher spending scores than older customers, but again the correlation is very weak.
 * Annual Income and Spending Score:
    * There is a weak positive correlation between annual income and spending score (around 0.01).
    * This means that there might be a slight tendency for customers with higher incomes to also have higher spending scores, but the correlation is very weak.
* It’s important to note that correlation does not necessarily equal causation. Just because there is a weak positive correlation between income and spending score does not mean that having a higher income causes people to spend more money. There could be other factors that explain the relationship.

The below code generates a violin plot to visualize the distribution of spending scores among different genders. The violin plot provides information about the central tendency, spread, and shape of the distribution for each gender category
"""

plt.figure(figsize=(10, 6))
sns.violinplot(x='Gender', y='Spending Score (1-100)', data=data)
plt.title('Violinplot of Spending Score by Gender')
plt.show()

"""Based on the violin plot,
* It appears that there is a significant difference in the spending score between males and females.
* The males have a higher spending score than the females, and the females have a lower spending score than the males.

The below code snippet aims to pre-process a pandas dataframe by selecting only the numeric features and optionally removing a specific column (CustomerID) before it can be used for Multidimensional Scaling (MDS).
"""

# Extract numeric features for MDS
numeric_features = data.select_dtypes(include=[np.number]).drop(columns=['CustomerID'])

# Calculate pairwise distances (dissimilarities) between objects using numeric features only
dist_euclid = pairwise_distances(numeric_features, metric='euclidean')
dist_manhattan = pairwise_distances(numeric_features, metric='manhattan')

"""In essence,
* The above code calculates two different distance metrics (Euclidean and Manhattan) between all pairs of data points based solely on their numeric features.
* This provides you with two ways to measure the similarity or dissimilarity between data points in the high-dimensional numeric feature space.
"""

# Labels for annotation
customer_info = data['CustomerID']

"""In essence,
* The above code retrieves customer IDs from a larger dataset and stores them in a separate variable for potential further use in your analysis.
* This could be useful for labeling data points or keeping track of customer information alongside the analysis of other features in the dataframe.
"""

# Helper function for visualization
def visualize_embeddings(pts, labels, title):
    fig = plt.figure(figsize=(15, 6))
    ax1 = fig.add_subplot(1, 2, 1)
    ax2 = fig.add_subplot(1, 2, 2)

    sns.scatterplot(x=pts[:, 0], y=pts[:, 1], hue=labels, palette=['r', 'g', 'b', 'y'], ax=ax1)
    ax1.set_title(title + ' - Colored by Class')

    ax2.scatter(pts[:, 0], pts[:, 1])
    for x, ind in zip(numeric_features.values, range(pts.shape[0])):
        im = x.reshape(1, -1)
        imagebox = OffsetImage(im, zoom=0.3, cmap=plt.cm.gray)
        i = pts[ind, 0]
        j = pts[ind, 1]
        ab = AnnotationBbox(imagebox, (i, j), frameon=False)
        ax2.add_artist(ab)
    ax2.set_title(title + ' - Annotated by Customer Info')
    plt.show()

"""The above code explains,
* Function Definition:The function visualize_embeddings is defined, taking three arguments: pts (the embeddings), labels (labels corresponding to the embeddings), and title (title for the visualization).
* Creating Figure and Subplots:The function initializes a figure with a specific size and creates two subplots: one for the scatter plot colored by class and another for annotating data points with additional information.
* Scatter Plot (Subplot 1):In the first subplot, a scatter plot is created using seaborn's scatterplot function, with embeddings plotted against each other (x=pts[:, 0], y=pts[:, 1]).
The points are colored based on the provided labels, using a specified color palette.
The title of the subplot is set.
* Annotated Scatter Plot (Subplot 2):In the second subplot, a scatter plot is created using matplotlib's scatter function.
For each data point in the scatter plot, an annotation box is added containing additional information (presumably numeric features) represented as images.
* Display:The function displays the figure containing both subplots using plt.show().
* Overall, the function allows for the visualization of embeddings along with their labels and additional information in two subplots, providing a comprehensive view of the data.

* The below code snippet performs Multidimensional Scaling (MDS) using Euclidean distance as the dissimilarity measure.

* Initialization:It initializes an MDS object (mds_euclid) with parameters specifying that dissimilarities are precomputed and sets the random state to ensure reproducibility.
* Transformation:It applies MDS transformation to the precomputed dissimilarity matrix (dist_euclid) using the fit_transform method.
This step projects the data into a lower-dimensional space while preserving the pairwise Euclidean distances as much as possible.
* Visualization:It then calls the visualize_embeddings function to visualize the transformed embeddings (pts_euclid) along with additional information (customer_info) under the title 'MDS with Euclidean Distance'.
* In short, this code performs MDS to reduce the dimensionality of the data while preserving the Euclidean distances, and then visualizes the transformed embeddings.
"""

# Apply MDS with Euclidean distance
mds_euclid = MDS(dissimilarity='precomputed', random_state=0)
pts_euclid = mds_euclid.fit_transform(dist_euclid)
visualize_embeddings(pts_euclid, customer_info, 'MDS with Euclidean Distance')

"""Based on the graph,
* It appears to be a visualization of customer data projected into a two-dimensional space, likely created using dimensionality reduction techniques like Multidimensional Scaling (MDS).
* The data points are colored according to a categorical variable (possibly customer segment or purchase category), and there are small image overlays on some of the data points.

* Here's a breakdown of the key elements in the image:

 * Scatter plot:
    * The main body of the image shows a scatter plot where each dot represents a customer.
    * The x and y axes likely correspond to two dimensions extracted during dimensionality reduction, capturing important variations in the data with just two values.
 * Coloring by category:
    * The data points are colored differently based on the values in a categorical variable.
    * The legend in the top left corner shows four colors (red, green, blue, and yellow) corresponding to four different categories (which could be customer segments, product types, or other classifications).
 * Image overlays:
   * Some data points have small image overlays. These images likely correspond to the rows in the numeric_features dataframe associated with each customer.
* Overall, this visualization seems to be designed to explore how customers are distributed in the lower-dimensional space based on their category and potentially their underlying numeric features. By looking at the plot, you might be able to see if there are clusters of customers belonging to specific categories, or if there are any interesting patterns in how customer features are visualized with the image overlays

* The below code segment applies Multidimensional Scaling (MDS) using Manhattan distance as the dissimilarity measure.

* Initialization:
   * It initializes an MDS object (mds_manhattan) with parameters specifying that dissimilarities are precomputed and sets the random state to ensure reproducibility.
* Transformation:
  * It applies MDS transformation to the precomputed dissimilarity matrix (dist_manhattan) using the fit_transform method.
  * This step projects the data into a lower-dimensional space while preserving the pairwise Manhattan distances as much as possible.
* Visualization:
  * It then calls the visualize_embeddings function to visualize the transformed embeddings (pts_manhattan) along with additional information (customer_info) under the title 'MDS with Manhattan Distance'.
* In short, this code performs MDS to reduce the dimensionality of the data while preserving the Manhattan distances, and then visualizes the transformed embeddings.
"""

# Apply MDS with Manhattan distance
mds_manhattan = MDS(dissimilarity='precomputed', random_state=0)
pts_manhattan = mds_manhattan.fit_transform(dist_manhattan)
visualize_embeddings(pts_manhattan, customer_info, 'MDS with Manhattan Distance')

"""The below code segment performs the following tasks:

* Setting Up Cluster Labels:
  * It sets up random cluster labels using np.random.randint function to assign each data point to one of four clusters.
* Applying MDS:
  * It initializes an MDS object (mds) specifying the number of components (2 for two-dimensional embedding), dissimilarity measure as precomputed, and setting the random state for reproducibility.
  * It then applies MDS transformation to the precomputed dissimilarity matrix (distances) using the fit_transform method.
* Plotting MDS Embeddings with Clustering:
  * It creates a scatter plot of the transformed MDS embeddings (pts) where each point is colored according to its cluster label (c=cluster_labels) using the 'viridis' colormap.
  * It sets the title, x-axis label, y-axis label, and adds a colorbar to represent cluster labels.
  * Finally, it displays the plot using plt.show().
* In short, this code generates a scatter plot to visualize the MDS embeddings of the data points while coloring them based on the randomly assigned cluster labels.







"""

np.random.seed(42)
num_clusters = 4
cluster_labels = np.random.randint(0, num_clusters, size=len(data))

# Apply MDS
mds = MDS(n_components=2, dissimilarity='precomputed', random_state=0)
pts = mds.fit_transform(distances)

# Plot MDS Embeddings with Clustering
plt.figure(figsize=(8, 6))
plt.scatter(pts[:, 0], pts[:, 1], c=cluster_labels, cmap='viridis')
plt.title('MDS Embeddings with Clustering')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.colorbar(label='Cluster Label')
plt.show()

"""**Insight from Customer Segmentation Visualization**
* A visualization of mall customer data projected into two dimensions using Multidimensional Scaling (MDS) reveals patterns in customer segmentation and feature distribution.

* ***Key Findings:***

  * **Customer Segmentation:**
     * The data points are colored according to a categorical variable, likely representing customer segments.
     * By observing the distribution of colors in the two-dimensional space, we can identify potential customer segments with similar characteristics.
     * For example, clusters of specific colors far from others might indicate distinct segments.
  * ***Visualizing Customer Features:***
    * Small image overlays on some data points likely represent the corresponding customer's numeric features.
    * These features might be reshaped and visualized to explore potential correlations between features and customer segments.
    * If certain image patterns are visually associated with specific colors (categories), it could suggest relationships between the features and the segmentation.
  * ***Considerations:***
     * Interpreting these visualizations effectively requires considering the context and domain knowledge of the data.
     *  What do the different colors represent? How do the numeric features translate to the image overlays?
* Overall, this visualization provides valuable insights into customer segmentation and helps us explore the relationship between customer characteristics and their categorization.
"""