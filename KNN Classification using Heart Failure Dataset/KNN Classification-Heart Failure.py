# -*- coding: utf-8 -*-
"""2348409_Lab05.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aYAC_x1k_2jCgFwtdduz-Ifs50Nw9B_s

**Lab 05- KNN Classification**

* *Created by: Angeline A*
* *Reg No: 2348409*

* Edited on: 21/03/2024
* Submitted on: 21/03/2024

In the below code, we are importing the necessary packages for performing the KNN Classification.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""We are importing the Kaggle dataset named heart.csv which contains information about the patients heart failure/success rate based on certain features.

**1. Loading the Dataset into the environment**
"""

data = pd.read_csv("/content/heart.csv")
data

"""**2. DISPLAYING THE BASIC INFORMATION**

The below code provides a quick overview of the dataset, including the number of entries, the number of columns, and the data types of each column. It's useful for understanding the structure and characteristics of the dataset before further analysis.
"""

# Display basic information about the dataset
print("Basic Information about the Dataset:")
print(data.info())

"""From the above result, we can infer certain things that are as follows:
* Size of the Dataset:
 * The dataset contains 918 entries (rows) and 12 columns.
* Data Types:
 * Most columns are of integer type (int64), representing numerical data.
 * One column is of float type (float64), likely representing continuous numerical data.
 * Several columns are of object type (object), indicating categorical data or text data.
* Memory Usage:
 * The memory usage of the dataset is relatively small, around 86.2 KB.
* Completeness of Data:
 * There are no missing values (non-null count is equal to the total number of entries) in any of the columns.
* Potential Categorical Features:
 * Columns such as 'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', and 'ST_Slope' are likely categorical variables based on their data types.
* Target Variable:
 * The column 'HeartDisease' appears to be the target variable, as it has integer values and represents a potential target for classification analysis.

Overall, the dataset seems to be well-structured, with no missing values and a mix of numerical and categorical features. It appears suitable for further analysis, such as predictive modeling or exploratory data analysis.

The below code provides a quick summary of the dimensions of the dataset, showing the number of samples (rows) and features (columns).
"""

# Display the number of samples and features
num_samples, num_features = data.shape
print("\nNumber of samples:", num_samples)
print("Number of features:", num_features)

"""From the above result we can infer the following informations:
* Number of Samples (Rows):
 * There are 918 samples in the dataset. Each sample represents a single observation or instance in the dataset.
* Number of Features (Columns):
 * There are 12 features in the dataset.
 * Each feature represents a characteristic or attribute associated with each sample.

This information provides a basic understanding of the size and structure of the dataset. It indicates that there are a considerable number of observations (samples) and a moderate number of attributes (features) available for analysis.
"""

# Display data types of features
print("\nData Types of Features:")
print(data.dtypes)

"""**3. First few rows to understand the structure and format of the data.**"""

# Display the first few rows
print("\nFirst Few Rows of the Dataset:")
data.head()

"""The above result displays the first five rows of the dataset heart.csv.

**4. UNIVARIATE ANALYSIS**
"""

# Numerical variables
numerical_variables = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']

"""In the above code, we are seperating the Numerical values from the dataset.

a. Calculate basic descriptive statistics (mean, median, mode, standard deviation,
min, max, quartiles, etc.).

The below code provides essential summary statistics for the numerical variables in the dataset, aiding in understanding the distribution and characteristics of the data.
"""

# Calculate basic descriptive statistics
descriptive_stats = data[numerical_variables].describe()
print("Basic Descriptive Statistics:")
print(descriptive_stats)

"""From the above results we can infer the following:
* Age Distribution:
 * The average age of individuals in the dataset is approximately 53.51 years, with a standard deviation of about 9.43 years.
 * The ages range from 28 years to 77 years.
 * The majority of individuals (approximately 50%) fall between the ages of 47 and 60, as indicated by the 25th and 75th percentiles.
* Blood Pressure and Heart Rate:
 * The average resting blood pressure is approximately 132.40 mmHg, with a standard deviation of about 18.51 mmHg.
 * The maximum resting blood pressure observed is 200 mmHg, while the minimum is 0 mmHg, which might be an outlier or missing value.
 * The average maximum heart rate is approximately 136.81 beats per minute (bpm), with a standard deviation of about 25.46 bpm.
 * The maximum heart rate ranges from 60 bpm to 202 bpm.
* Cholesterol Levels:
 * The average cholesterol level is approximately 198.80 mg/dL, with a standard deviation of about 109.38 mg/dL.
 * The cholesterol levels vary widely, ranging from 0 mg/dL to 603 mg/dL.
* ST Depression:
 * The average ST depression induced by exercise relative to rest (Oldpeak) is approximately 0.89.
 * The Oldpeak values range from -2.6 to 6.2, indicating variations in the extent of ST depression across individuals.

b. Visualize the distribution using histograms, kernel density plots, or box plots.

The below code generates the histogram which shows us how the data points are distributed.
"""

# Visualize the distribution using histograms
print("\nHistograms for Numerical Variables:")
data[numerical_variables].hist(figsize=(12, 10))
plt.tight_layout()
plt.show()

"""From the above results, we can infer Age and MaxHR are distributed normally and the other values are not distributed normally.

The below code is generating the kernal density plot which shows us how the data points are distributed
"""

# Visualize the distribution using kernel density plots
print("\nKernel Density Plots for Numerical Variables:")
for var in numerical_variables:
    sns.displot(data[var], kind='kde')
plt.show()

"""From the above kernel density plot, we can see that the data points of Age amd MaxHR is distributed normally.

The below code generates box plots to visualize the distribution and variability of numerical variables in the dataset, allowing for quick assessment of data spread, central tendency, and presence of outliers.
"""

# Visualize the distribution using box plots
print("\nBox Plots for Numerical Variables:")
plt.figure(figsize=(12, 8))
sns.boxplot(data=data[numerical_variables])
plt.xticks(rotation=45)
plt.show()

"""From the above graph, we can infer that there are noticible outliers in Cholesterol and RestingBP.

**FOR CATEGORICAL VARIABLES**
a. Display frequency tables showing counts and percentages.

In the below code, we are seperating the categorical variables seperately.
"""

# Categorical variables
categorical_variables = ['Sex', 'ChestPainType', 'FastingBS', 'RestingECG', 'ExerciseAngina']

"""The below code generates frequency tables for each categorical variable, providing insights into the distribution of categorical data in the dataset."""

# Display frequency tables showing counts and percentages
print("Frequency Tables for Categorical Variables:")
for var in categorical_variables:
    print("\n", var)
    print(data[var].value_counts(normalize=True) * 100)

"""From the above code we can infer that,
* Sex:
 * The majority of individuals in the dataset are male (M), accounting for approximately 79% of the total, while females (F) represent around 21%.
* Chest Pain Type:
 * Most individuals experience atypical angina (ASY), accounting for approximately 54% of cases.
 * Non-anginal pain (NAP) and typical angina (ATA) are less common, representing approximately 22% and 19% of cases, respectively.
 * A very small proportion of individuals have asymptomatic chest pain (TA), accounting for around 5% of cases.
* Fasting Blood Sugar (FastingBS):
 * The majority of individuals have fasting blood sugar levels below 120 mg/dL (0), accounting for approximately 77% of cases.
 * Around 23% of individuals have fasting blood sugar levels equal to or above 120 mg/dL (1).
* Resting Electrocardiographic Results (RestingECG):
 * Most individuals have a normal resting electrocardiogram (ECG), representing approximately 60% of cases.
 * A significant proportion of individuals exhibit left ventricular hypertrophy (LVH), accounting for around 20% of cases.
 * Around 19% of individuals show ST-T wave abnormality (ST) in their resting ECG.
* Exercise-Induced Angina (ExerciseAngina):
 * Approximately 59.6% of individuals do not experience exercise-induced angina (N), while around 40.4% of individuals do experience it (Y).

These insights provide valuable information about the distribution and prevalence of categorical variables in the dataset, which can help in understanding the characteristics of the study population and identifying potential patterns or associations with the target variable.

b. Visualize using bar plots.

The below code generates bar plots for each categorical variable, providing visual representations of the frequency distribution of categories within each variable. It helps in understanding the distribution of categorical data and identifying any potential patterns or trends.
"""

# Visualize using bar plots
print("\nBar Plots for Categorical Variables:")
plt.figure(figsize=(12, 10))
for i, var in enumerate(categorical_variables, 1):
    plt.subplot(3, 2, i)
    sns.countplot(data=data, x=var)
    plt.title(var)
    plt.xlabel("")
    plt.ylabel("Count")
plt.tight_layout()
plt.show()

"""From the above result, we can see how the different categories are distributed in each of the features.

**5. BIVARIATE ANALYSIS**

a. Explore relationships between pairs of numerical variables using scatter plots
or pair plots.

The below code provides a visual overview of the relationships between pairs of numerical variables, allowing for the identification of potential patterns, correlations, and outliers in the data. Pair plots are useful for initial exploratory analysis in understanding the multivariate relationships within the dataset.
"""

# Bivariate Analysis

# Explore relationships between pairs of numerical variables using pair plots
print("\nPair Plots for Numerical Variables:")
sns.pairplot(data[numerical_variables])
plt.show()

"""b. Explore relationships between numerical and categorical variables using box
plots or violin plots.

The below code generates box plots to visualize the distribution of the numerical variable 'Age' across different categories of each categorical variable. It helps in understanding how the numerical variable varies with different categories of the categorical variable.
"""

# Explore relationships between numerical and categorical variables using box plots or violin plots
print("\nBox Plots for Numerical Variables by Categorical Variables:")
plt.figure(figsize=(12, 8))
for i, var in enumerate(categorical_variables, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(data=data, x=var, y='Age')  # You can choose any numerical variable here
    plt.title(f'Age by {var}')
    plt.xlabel(var)
    plt.ylabel('Age')
plt.tight_layout()
plt.show()

"""From the above graphs, we can infer how the features are distributed based on the age of the patients.

c. Calculate correlation coefficients between numerical variables.

The below code generates the correlation coefficient matrix which shows us the relationship between the features and tells us how they are related.
"""

# Calculate correlation coefficients between numerical variables
correlation_matrix = data[numerical_variables].corr()
print("\nCorrelation Matrix:")
print(correlation_matrix)

"""From the above result we can infer that,
* Age and Resting Blood Pressure (RestingBP):
 * There is a positive correlation of approximately 0.25 between age and resting blood pressure.
 * This suggests that as age increases, resting blood pressure tends to increase slightly.
* Age and Max Heart Rate (MaxHR):
 * There is a negative correlation of approximately -0.38 between age and max heart rate.
 * This indicates that as age increases, max heart rate tends to decrease.
* Resting Blood Pressure (RestingBP) and Max Heart Rate (MaxHR):
 * There is a weak negative correlation of approximately -0.11 between resting blood pressure and max heart rate.
 * This suggests a slight tendency for individuals with higher resting blood pressure to have lower max heart rates.
* Oldpeak and Age/RestingBP:
 * There is a positive correlation of approximately 0.26 between oldpeak (ST depression induced by exercise relative to rest) and age
 * This indicates a slight tendency for older individuals to have higher ST depression.
 * There is also a positive correlation of approximately 0.16 between oldpeak and resting blood pressure. This suggests a slight tendency for individuals with higher resting blood pressure to have higher ST depression.
* Cholesterol:
 * There is no strong correlation between cholesterol levels and other variables.
 * The correlation coefficients are close to zero, indicating weak or no linear relationship.

These insights help in understanding the relationships between different numerical variables in the dataset. However, it's essential to note that correlation does not imply causation, and further analysis may be required to uncover more complex relationships.

**6. Drop the non-required columns / features (dependent columns)**

**4 Classes**
* Typical Angina
* Atypical Angina
* Non-Anginal Pain
* Asymptotic

In the below code we are seperating the independent and dependent variables.
"""

X = data.drop('HeartDisease', axis=1)
y = data['HeartDisease']

"""In the below code, we are dropping the HeartDisease variable which is the dependent variable."""

dependent_columns = ['HeartDisease']  # Assuming 'HeartDisease' is the dependent column
data = data.drop(dependent_columns, axis=1)

"""**9. Perform Standardization:**
* i. Apply a specific Scalar based on the requirement to standardize the data
"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Separate numerical and categorical columns
numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns
categorical_columns = X.select_dtypes(include=['object']).columns

# Define preprocessing steps for numerical and categorical data separately
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Apply preprocessing steps to numerical and categorical columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_columns),
        ('cat', categorical_transformer, categorical_columns)
    ])

# Fit and transform data with preprocessing
X_processed = preprocessor.fit_transform(X)

"""10. Split the Training and Testing Dataset

In the below code, we are splitting dataset for training and testing.
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=4)

"""11. Model K-NN with different ‘K’ values and give your inference

The below code evaluates the performance of the KNN classifier using different values of k and prints the accuracy scores for each configuration. It helps in identifying the optimal value of k for the given dataset.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

k_values = [3, 5, 7, 10, 12, 15, 20, 25, 30]  # Example k values
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy with k={k}: {accuracy}")

"""From the provided accuracy scores for different values of k:

* The highest accuracy is achieved when k=15, with an accuracy of approximately 91.30%.
* Generally, as the value of k increases, the accuracy initially improves up to a certain point and then may start to decrease or plateau.
* This pattern suggests that a moderate number of neighbors (around k=15 in this case) may provide the best balance between bias and variance, resulting in higher accuracy.
* It's essential to choose an appropriate value of k to avoid overfitting (low k) or underfitting (high k) the model.
* The accuracy scores provide insights into the performance of the KNN classifier with different neighborhood sizes and can help in selecting the optimal value of k for this dataset.

12. Model the confusion matrix and display the correct and wrong predictions

In the below code, Each model is trained using a specific distance metric and then used to predict the labels for the test data. This allows for comparison of the performance of the KNN algorithm using different distance metrics.
"""

# Example: Euclidean distance
knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')
knn_euclidean.fit(X_train, y_train)
y_pred_euclidean = knn_euclidean.predict(X_test)

# Example: Manhattan distance
knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')
knn_manhattan.fit(X_train, y_train)
y_pred_manhattan = knn_manhattan.predict(X_test)

# Example: Cosine similarity
knn_cosine = KNeighborsClassifier(n_neighbors=5, metric='cosine')
knn_cosine.fit(X_train, y_train)
y_pred_cosine = knn_cosine.predict(X_test)

""" The below code facilitates the comparison of the performance of KNN models using different distance metrics by visualizing their confusion matrices."""

from sklearn.metrics import confusion_matrix

# Calculate confusion matrix for each method
conf_matrix_euclidean = confusion_matrix(y_test, y_pred_euclidean)
conf_matrix_manhattan = confusion_matrix(y_test, y_pred_manhattan)
conf_matrix_cosine = confusion_matrix(y_test, y_pred_cosine)

# Plot confusion matrices
plt.figure(figsize=(18, 6))

# Plot confusion matrix for Euclidean distance
plt.subplot(1, 3, 1)
sns.heatmap(conf_matrix_euclidean, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.title('Confusion Matrix (Euclidean)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Plot confusion matrix for Manhattan distance
plt.subplot(1, 3, 2)
sns.heatmap(conf_matrix_manhattan, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.title('Confusion Matrix (Manhattan)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Plot confusion matrix for Cosine similarity
plt.subplot(1, 3, 3)
sns.heatmap(conf_matrix_cosine, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.title('Confusion Matrix (Cosine)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

plt.tight_layout()
plt.show()

"""From the above result we can infer how the methods are predicting the values correctly. The confusion matrix shows us how many correct predictions and wrong predictions made by each of the methods.

The below code show us the visualization for accuracy rate for each of the methods.
"""

import matplotlib.pyplot as plt

# Define different distance metrics
distance_metrics = ['euclidean', 'manhattan', 'cosine']

# Initialize empty lists to store accuracy scores
accuracy_scores = []

# Iterate over each distance metric
for metric in distance_metrics:
    # Create and train K-NN model with current metric
    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)
    knn.fit(X_train, y_train)

    # Predict on test set
    y_pred = knn.predict(X_test)


    # Calculate accuracy score and append to list
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)

# Plot accuracy scores for different metrics
plt.figure(figsize=(8, 6))
plt.bar(distance_metrics, accuracy_scores, color='skyblue')
plt.title('Accuracy Scores for Different Distance Metrics')
plt.xlabel('Distance Metric')
plt.ylabel('Accuracy Score')
plt.ylim(0, 1)
plt.show()

"""From the above graph we can infer that the model is predicting more accurate with cosine method than the other methods euclidean and manhattan.

The below code snippet allows you to compare the performance of KNN models using different distance metrics by providing detailed classification reports for each model.
"""

from sklearn.metrics import classification_report

print("Classification Report (Euclidean):")
print(classification_report(y_test, y_pred_euclidean))

print("Classification Report (Manhattan):")
print(classification_report(y_test, y_pred_manhattan))

print("Classification Report (Cosine):")
print(classification_report(y_test, y_pred_cosine))

"""From  the above results
* Accuracy:
 * All three methods have similar accuracy levels, with scores ranging from 0.88 to 0.89.
 * Since the accuracy is comparable, it doesn't provide a clear distinction between the methods.

* Precision, Recall, and F1-score:
 * We should examine these metrics for each class and consider which metric is most important based on the specific context of the classification task.
 * For example, if correctly identifying instances of one class (high recall) is more critical than avoiding false positives (high precision), then we might prioritize recall.
 * Similarly, if both precision and recall are equally important, we might consider the F1-score, which balances both metrics.

* Domain Knowledge:
 * Understanding the domain and the nature of the features in the dataset can provide insights into which distance metric might be more appropriate.
 * For example, if the features are naturally aligned with Euclidean space (e.g., numerical features), then the Euclidean distance might be more suitable.
 * On the other hand, if the features represent vectors or documents, Cosine similarity might be more appropriate.

* Robustness to Outliers and Feature Scaling:
 * Euclidean distance is sensitive to the scale of features and outliers, while Manhattan distance is less sensitive to outliers.  

  * Cosine similarity is robust to the scale of features but ignores magnitude and focuses on the orientation of vectors. Depending on the characteristics of the dataset, one method might be more robust than the others.

Given these considerations, it's challenging to definitively declare one method as universally better than the others without deeper analysis and understanding of the specific dataset and classification task. It might be beneficial to experiment with different distance metrics and evaluate their performance using cross-validation or other validation techniques to make a more informed decision. Additionally, considering the trade-offs between different metrics and the specific requirements of the application can help in selecting the most appropriate method for KNN classification.

The below code for heatmaps provide a visual comparison of the performance of K-NN models with different distance metrics in terms of correct and wrong predictions. This visualization helps in understanding how each distance metric affects the model's predictive performance.
"""

import numpy as np

# Initialize empty lists to store correct and wrong predictions for each method
correct_predictions = []
wrong_predictions = []

# Iterate over each K-NN model with different distance metrics
for knn_model in [knn_euclidean, knn_manhattan, knn_cosine]:
    # Predict on test set
    y_pred = knn_model.predict(X_test)

    # Calculate number of correct predictions
    correct = np.sum(y_pred == y_test)
    correct_predictions.append(correct)

    # Calculate number of wrong predictions
    wrong = np.sum(y_pred != y_test)
    wrong_predictions.append(wrong)

# Convert lists to numpy arrays for heatmap plotting
correct_predictions = np.array(correct_predictions).reshape(1, -1)
wrong_predictions = np.array(wrong_predictions).reshape(1, -1)

# Plotting heatmap for correct predictions
plt.figure(figsize=(10, 4))
sns.heatmap(correct_predictions, annot=True, cmap='Greens', fmt='d', xticklabels=distance_metrics, yticklabels=False)
plt.title('Correct Predictions by K-NN with Different Distance Metrics')
plt.xlabel('Distance Metric')
plt.ylabel('Correct Predictions')
plt.show()

# Plotting heatmap for wrong predictions
plt.figure(figsize=(10, 4))
sns.heatmap(wrong_predictions, annot=True, cmap='Reds', fmt='d', xticklabels=distance_metrics, yticklabels=False)
plt.title('Wrong Predictions by K-NN with Different Distance Metrics')
plt.xlabel('Distance Metric')
plt.ylabel('Wrong Predictions')
plt.show()

"""From the above visualization we can infer that
* The euclidean distance method is predicting 163 values correctly and making wrong predictions for 21 values.
* The manhattan Distance method is making 162 correct predictions and 22 wrong predictions.
* The Cosine distance method is making 164 correct predictions and 20 wrong predictions.

The below code for bar plot provides a visual comparison of the number of correct and wrong predictions made by K-NN models using different distance metrics. This visualization helps in understanding how each distance metric performs in terms of prediction accuracy.
"""

# Create bar plot for correct predictions
plt.figure(figsize=(10, 6))
plt.bar(distance_metrics, correct_predictions.flatten(), color='lightgreen', label='Correct Predictions')

# Create bar plot for wrong predictions
plt.bar(distance_metrics, wrong_predictions.flatten(), bottom=correct_predictions.flatten(), color='salmon', label='Wrong Predictions')

plt.title('Correct vs. Wrong Predictions by K-NN with Different Distance Metrics')
plt.xlabel('Distance Metric')
plt.ylabel('Number of Predictions')
plt.legend()
plt.show()

"""From the above image/graph,
* we can say that the better prediction is made by Cosine distance method.
* Considering all the factors like random_state and the K-Nearest neighnours, we get the more accuracy rate for the method cosine distance method.
"""