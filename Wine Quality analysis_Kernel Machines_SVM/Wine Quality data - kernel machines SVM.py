# -*- coding: utf-8 -*-
"""2348409_Lab9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dQV6YNNw_FiSbf2Qiza_0qQl-tihtGd8

**Kernal Machines / SVM**

***LAB 09***
* *Created by: Angeline A*
* *Reg No: 2348409*
* *3-MDS 'B'*
* Submitted on: 11/04/2024

In the below code,
* We are importing the library pandas which is used for data manipulation and analysis.
* We are reading the data and storing it in the variable wine_data.
* The dataset basically has the information about red wine and it's quality and the level of constituents which is contributing to the qulaity of the wine.
"""

import pandas as pd

# Load the dataset
wine_data = pd.read_csv("/content/winequality-red.csv")

"""In the below code we are retrieving the number of samples which basically the number of rows present in the dataset and the features represents the number of features which is basically the number of columns present in the dataset."""

# Display basic information about the dataset
print("Number of samples:", wine_data.shape[0])
print("Number of features:", wine_data.shape[1])

"""From the above result, we can see that they are 1599 samples (rows) in the dataset and 12 features (columns) in the dataset.

In the below code, we are retrieving the datatypes of all the features present in the dataset.
"""

print("\nData types of features:")
print(wine_data.dtypes)

"""* The output of print(wine_data.dtypes) provides a concise summary of the data types of each feature in the Red Wine Quality dataset.
* All features are numeric, with most being of type float64 (floating-point numbers representing decimal values) and one feature, quality, being of type int64 (integer values representing quality ratings).

The below code snippet retrieves the first five rows of the dataset.
"""

# Display the first few rows of the dataset
print("\nFirst few rows of the dataset:")
wine_data.head()

"""In the below code, we are importing the libraries like
* numpy which is used for mathematical calculations
* matplotlib.pyplot and seaborn used for visulization.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""In the below code, we are storing the numerical varibles in a seperate variable and we are calculating the descriptive statistics for the numerical variables and storing it in a variable."""

# Select numerical variables
numerical_variables = wine_data.select_dtypes(include=np.number)

# Calculate basic descriptive statistics
descriptive_statistics = numerical_variables.describe()

""" The below code generates histograms for each numerical variable in the dataset, allowing you to visualize the distribution of each variable's values."""

# Visualize the distribution using histograms
numerical_variables.hist(figsize=(15, 10), bins=20, color='skyblue', edgecolor='black')
plt.suptitle("Histograms of Numerical Variables", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""Here's a brief description of each histogram:

* Fixed Acidity: Most values are clustered around 6 to 8, with the distribution tapering off towards higher values.
* Volatile Acidity: The distribution is somewhat bell-shaped, with a peak around 0.5 to 0.6.
* Citric Acid: The distribution is skewed to the right, with a concentration of values close to 0 and fewer high values.
* Residual Sugar: The histogram shows a right-skewed distribution with most values concentrated at the lower end.
* Chlorides: This histogram is also right-skewed, with most of the data points falling below 0.1.
* Free Sulfur Dioxide: The distribution is right-skewed, with a long tail extending to higher values.
* Total Sulfur Dioxide: Similar to free sulfur dioxide, this histogram is right-skewed with a long tail.
* Density: The distribution appears to be roughly bell-shaped, centered around 0.996 to 0.998.
* pH: The histogram shows a distribution that is approximately normally distributed, centered around a pH of 3.2 to 3.4.
* Sulphates: The distribution is somewhat bell-shaped but slightly skewed to the right, with most values between 0.5 and 0.75.
* Alcohol: The distribution is slightly left-skewed, with a concentration of values between 9 and 10.
* Quality: This histogram shows a multimodal distribution with peaks at quality ratings of 5, 6, and 7.
* From these histograms,
  * We can infer that many of the variables have a skewed distribution, which is common in real-world data.
  * The variables related to sulfur dioxide (both free and total) show a significant right skew, indicating that there are a number of wines with high sulfur dioxide content, but the majority have lower levels.
  * The quality variable shows that most wines have a quality rating around 5 or 6, with fewer wines rated 7 or 8, and very few at the lower end of the scale (3 or 4).

The below code generates kernel density plots for each numerical variable in the dataset, allowing you to visualize the distribution of each variable's values using smoothed curves.
"""

# Visualize the distribution using kernel density plots (KDE)
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_variables.columns):
    plt.subplot(3, 4, i + 1)
    sns.kdeplot(data=numerical_variables[col], shade=True, color='skyblue')
    plt.title(col)
plt.suptitle("Kernel Density Plots of Numerical Variables", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""From the above graph we can infer the following,
* Fixed Acidity: This plot shows a distribution that is slightly right-skewed, indicating that most of the wines have a moderate level of fixed acidity, with fewer examples of very high acidity.

* Volatile Acidity: The distribution is also right-skewed, similar to fixed acidity, but with a more pronounced tail, suggesting that while most wines have lower levels of volatile acidity, there's a long tail of wines with higher volatile acidity.

* Citric Acid: This distribution appears to be multi-modal, with a few peaks. This could mean that the wines can be grouped into categories based on their citric acid content.

* Residual Sugar: Here, we see a distribution with a significant right-skew, with most wines having low levels of residual sugar, while a few have a very high sugar content.

* Chlorides: The plot indicates a distribution with a very long right tail. The majority of samples have low chloride content, with very few samples having high chloride content.

* Free Sulfur Dioxide: The distribution here is right-skewed, with most wines having a low to moderate amount of free sulfur dioxide.

* Total Sulfur Dioxide: The pattern is similar to free sulfur dioxide, again right-skewed, suggesting that while most wines have a lower total sulfur dioxide content, there are wines with significantly higher amounts.

* Density: This plot presents a distribution that is somewhat symmetric, but with a slight left-skew, indicating that the variable is densely packed around a central value with few low outliers.

* pH: The pH distribution seems fairly normal (Gaussian) with a slight left-skew. It represents that most wines have a pH that centers around a particular value with some variation on both the lower and higher ends.

* Sulphates: The distribution here is right-skewed, with a tail stretching towards higher sulphate values, indicating that most wines have lower sulphate content.

* Alcohol: The alcohol content shows a distribution that is slightly left-skewed, indicating that there are more high alcohol contents than low, with a concentration of values at the higher end.

* Quality: This appears to be the quality score of the wines. The distribution is multi-modal with peaks suggesting that there are several distinct levels of quality that are common within the dataset.

The below code generates box plots for each numerical variable in the dataset, allowing you to visualize the distribution of each variable's values and identify any outliers or variability.
"""

# Visualize the distribution using box plots
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_variables.columns):
    plt.subplot(3, 4, i + 1)
    sns.boxplot(y=numerical_variables[col], color='skyblue')
    plt.title(col)
plt.suptitle("Box Plots of Numerical Variables", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""Here is the insight for each of the features:

* Fixed Acidity: The median fixed acidity is roughly between 7 and 8 grams per deciliter. The data is spread evenly around the median, with a few outliers on the higher end.

* Volatile Acidity: This feature has a median value just below 0.5 g/dL. The data is right-skewed with some outliers indicating occasional high volatile acidity.

* Citric Acid: The median citric acid content is around 0.25 g/dL. The distribution is skewed towards lower values with some outliers on the higher end.

* Residual Sugar: This feature has a wide range of values, with a median below 3 g/dL but a long tail of outliers reaching towards 15 g/dL, indicating that most wines have low residual sugar with some exceptions.

* Chlorides: The median chloride content is small, below 0.1 g/dL, with a symmetric distribution and a few higher outliers.

* Free Sulfur Dioxide: There is a right-skewed distribution with a median near 15 mg/L. The data contains numerous outliers, indicating episodes of much higher concentrations.

* Total Sulfur Dioxide: This characteristic is also right-skewed, with a median about 40 mg/L, and has outliers that suggest some wines have significantly higher total sulfur dioxide levels.
* Density: The density feature shows a relatively tight grouping around the median, which is very close to 1.000 g/cmÂ³, with no significant outliers.

* pH: pH values have a median around 3.2 and are symmetrically distributed. There are a few outliers on both the lower and higher sides.

* Sulphates: Median sulphate concentration is just above 0.5 g/L with a few outliers indicating higher concentrations than typical.

* Alcohol: This feature has a median value just below 10% volume, with a right-skewed distribution showing a tail towards higher alcohol contents.

* Quality: This seems to represent a score or rating. The median is around 6 on a scale that appears to go from 3 to 8. The distribution is approximately symmetric with some outliers on the lower end.

The below code snippet prints out the basic descriptive statistics of numerical variables in the dataset.
"""

# Print basic descriptive statistics
print("Basic Descriptive Statistics of Numerical Variables:")
print(descriptive_statistics)

"""Here's an explanation of the basic descriptive statistics provided for each feature in the dataset,
* Fixed Acidity:
The fixed acidity of the wine samples has a mean of approximately 8.32, with a standard deviation of around 1.74. The values range from a minimum of 4.6 to a maximum of 15.9, with a median value of 7.9.

* Volatile Acidity:
The volatile acidity of the wine samples has a mean of approximately 0.53, with a standard deviation of around 0.18. The values range from a minimum of 0.12 to a maximum of 1.58, with a median value of 0.52.

* Citric Acid:
The citric acid content of the wine samples has a mean of approximately 0.27, with a standard deviation of around 0.19. The values range from a minimum of 0.0 to a maximum of 1.0, with a median value of 0.26.

* Residual Sugar:
The residual sugar content of the wine samples has a mean of approximately 2.54, with a standard deviation of around 1.41. The values range from a minimum of 0.9 to a maximum of 15.5, with a median value of 2.2.

* Chlorides:
The chlorides content of the wine samples has a mean of approximately 0.09, with a standard deviation of around 0.05. The values range from a minimum of 0.012 to a maximum of 0.611, with a median value of 0.079.

* Free Sulfur Dioxide:
The free sulfur dioxide content of the wine samples has a mean of approximately 15.87, with a standard deviation of around 10.46. The values range from a minimum of 1 to a maximum of 72, with a median value of 14.

* Total Sulfur Dioxide:
The total sulfur dioxide content of the wine samples has a mean of approximately 46.47, with a standard deviation of around 32.90. The values range from a minimum of 6 to a maximum of 289, with a median value of 38.

* Density:
The density of the wine samples has a mean of approximately 0.997, with a standard deviation of around 0.002. The values range from a minimum of 0.990 to a maximum of 1.004, with a median value of 0.997.

* pH:
The pH level of the wine samples has a mean of approximately 3.31, with a standard deviation of around 0.15. The values range from a minimum of 2.74 to a maximum of 4.01, with a median value of 3.31.

* Sulphates:
The sulphates content of the wine samples has a mean of approximately 0.66, with a standard deviation of around 0.17. The values range from a minimum of 0.33 to a maximum of 2.0, with a median value of 0.62.

* Alcohol:
The alcohol content of the wine samples has a mean of approximately 10.42, with a standard deviation of around 1.07. The values range from a minimum of 8.4 to a maximum of 14.9, with a median value of 10.2.

* Quality:
The quality rating of the wine samples has a mean of approximately 5.64, with a standard deviation of around 0.81. The values range from a minimum of 3 to a maximum of 8, with a median value of 6.

The below imports pearsonr which is a function used for calculating the Pearson correlation coefficient and its associated p-value between two continuous variables in Python.
"""

from scipy.stats import pearsonr

"""The below code generates a grid of scatter plots for each pair of numerical variables, allowing for visual inspection of their relationships and distributions. Additionally, KDE plots along the diagonal provide a smoothed representation of each variable's distribution."""

# Explore relationships between pairs of numerical variables using scatter plots or pair plots
sns.pairplot(wine_data, vars=numerical_variables.columns, kind='scatter', diag_kind='kde')
plt.suptitle("Pair Plots of Numerical Variables", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""Here's an analysis of some features and insights from this image:

* Histograms:
  * On the diagonal, we see histograms, which show the distribution of a single variable.
  * The peaks, spread, and skewness of these histograms provides insight into the central tendency, variability, and distribution shape of each variable.

* Scatterplots:
  * The off-diagonal elements show scatterplots for all possible pairs of the variables.
  * These scatterplots help us understand the bivariate relationships between different variables.

* Correlation:
  * By looking at how points are spread on these scatterplots, one can get a sense of whether there's a correlation between the paired variables.
  * A clear linear pattern indicates a strong correlation, while a more diffuse cloud of points suggests a weaker correlation or none.

* Outliers:
  * Outlier points that don't fit the overall pattern of a scatterplot can be identified.
  * Outliers might suggest special cases, errors in data, or experimental extremes that are important for further analysis.

* Multivariate Relationships:
  * Some complex multivariate relationships might be suggested by a combination of scatterplots.
  * For instance, if variables A and B are correlated, and B and C are correlated, one might infer a relationship between A and C.

* Density:
  * The density of the plots, where more points are clustered, can also provide insights into the concentration and range of data points within the variable combinations.

The below code generates a grid of box plots, each showing the distribution of a numerical variable across different quality ratings of the wine samples. It allows for visual inspection of how each numerical variable varies with wine quality.
"""

# Explore relationships between numerical and categorical variables using box plots or violin plots
plt.figure(figsize=(15, 10))
for i, num_var in enumerate(numerical_variables.columns):
    plt.subplot(3, 4, i + 1)
    sns.boxplot(x='quality', y=num_var, data=wine_data, palette='viridis')
    plt.title("Box Plot of " + num_var + " by Quality")
plt.suptitle("Box Plots of Numerical Variables by Quality", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""Here is an explanation of each small graph, which are different box plots:

* Box Plot of fixed acidity by Quality: This plot shows the distribution of fixed acidity across the different quality categories. Fixed acidity appears to increase slightly with quality up to a certain point, then levels off or decreases.

* Box Plot of volatile acidity by Quality: There's a noticeable decrease in volatile acidity as the quality increases, with the median values trending downwards.

* Box Plot of citric acid by Quality: Citric acid levels seem to increase with the quality, showing that higher quality categories tend to have higher median values of citric acid.

* Box Plot of residual sugar by Quality: The spread of residual sugar is quite consistent across different qualities, though the median values do not show a clear trend with respect to quality.

* Box Plot of chlorides by Quality: There is a downward trend in the median values of chlorides with increasing quality. Higher quality categories display a lower median chloride level.

* Box Plot of free sulfur dioxide by Quality: The box plots show a fair amount of variability with no clear trend relating to quality in terms of the central tendency.

* Box Plot of total sulfur dioxide by Quality: Similar to free sulfur dioxide, there does not seem to be a clear trend in the median values with respect to quality, though there is a noticeable decrease in total sulfur dioxide at the highest quality level.

* Box Plot of density by Quality: Density seems to display a slight decrease in median values with rising quality, most noticeable at the highest levels of quality.

* Box Plot of pH by Quality: The pH shows variability across quality levels. The median pH doesn't seem to correspond with quality in a simple linear manner.

* Box Plot of sulphates by Quality: Sulphates seem to follow a slight increasing trend with higher quality categories, indicating higher sulphate levels might be associated with better quality.

* Box Plot of alcohol by Quality: There's a clear positive relationship between alcohol and quality, with higher levels of alcohol associated with higher quality levels.

* Box Plot of quality by Quality: This plot doesn't compare the quality with another variable but rather shows the spread of quality scores â essentially an overview of the quality distribution across samples.

In the below code, we are calculating the correlation co-efficients between the numerical variables and storing it in a variable.
"""

# Calculate correlation coefficients between numerical variables
correlation_matrix = numerical_variables.corr()

"""The below code generates a heatmap visualization of the correlation matrix, allowing for a quick and visual assessment of the relationships between numerical variables. Stronger correlations are represented by darker shades of blue or red, while weaker or no correlations are closer to white."""

# Plot correlation matrix heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix of Numerical Variables", fontsize=16)
plt.show()

"""In this matrix, the following insights can be observed:

* Diagonal Line of 1s: The diagonal from the top left to bottom right shows perfect correlation of 1 for each variable with itself, which is always the case.

* Positive Correlations:
  * There are several pairs of variables with strong positive correlations, where one variable tends to increase with the other.
  * For instance, "fixed acidity" has a relatively high positive correlation with "citric acid" (0.67).

* Negative Correlations:
  * Conversely, there are some pairs with negative correlations indicating that when one variable increases, the other tends to decrease.
  * An example is "fixed acidity" negatively correlating with "pH" (-0.68).

* Weak or No Correlation:
  * Some pairs of variables show very weak correlation close to 0, which suggests there is no strong linear relationship between them.
  * For example, "residual sugar" and "chlorides" have a correlation of 0.01.

* Heatmap Coloring:
  * The colors range from red through white to blue, representing the strength and direction of the correlation.
  * Red indicates a strong positive correlation, blue indicates a strong negative correlation, and white or light colors indicate weak or no correlation.

* Potential Multicollinearity:
  * If this data is used for predictive modeling, variables with high correlation may cause multicollinearity problems where it's hard to separate out the individual effects of collinear variables on the outcome variable.

* Variable Relationships:
  * Specific to the domain of this dataset, which appears to be wine analysis, the correlation between certain chemical properties can be relevant for predicting wine quality, flavor profiles, or other characteristics of interest.

In the below code, we are printing the correlation matrix
"""

# Print correlation coefficients
print("Correlation Coefficients between Numerical Variables:")
correlation_matrix

"""In the below code,
* we are importing necessary libraries for splitting the dataset into training and testing dataset and Standardizing the variables.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

"""In the below code snippet, we are splitting the independent variable and dependent variable and stoing it in the variable."""

# Step 8: Separate the features (X) and the target variable (y)
X = wine_data.drop(columns='quality')  # Features
y = wine_data['quality']               # Target variable

"""This code performs standardization on the features of the dataset using the StandardScaler from Scikit-learn to bring all the features to a similar scale which improves the performance and convergence of the model."""

# Step 9: Perform Standardization or normalization on the features as required
# We will perform standardization since it is often beneficial for models like SVM
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""In the below code snippet, we are splitiing the wine dataset into training and testing dataset."""

# Step 10: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=29)

"""In the below code, we are printing the shapes of the independent and dependent variables of the training and testing dataset."""

# Print the shapes of training and testing sets
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

"""From the above result, we can observe the shape of the training and testing dataset.

The below code snippet prepares the necessary tools for training a SVC model and evaluating its performance using classification metrics such as precision, recall, F1-score, confusion matrix, and accuracy score.
"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

"""The below code sets up three SVM models, each using a different kernel function (linear, polynomial, and RBF) to handle various types of data and capture different types of relationships between features."""

# Initialize SVM models with different kernel functions
svm_linear = SVC(kernel='linear')
svm_poly = SVC(kernel='poly')
svm_rbf = SVC(kernel='rbf')

"""The below lines of code train three SVM models with different kernel functions using the training data, allowing them to learn from the patterns present in the data for subsequent predictions."""

# Train SVM models using the training data
svm_linear.fit(X_train, y_train)
svm_poly.fit(X_train, y_train)
svm_rbf.fit(X_train, y_train)

"""The below function provides a comprehensive evaluation of each SVM model's performance on the test set, including accuracy, precision, recall, F1-score, and confusion matrix."""

# Evaluate the performance of each SVM model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy:", accuracy)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

"""The below code snippet evaluates the SVM model with a linear kernel function using the previously defined evaluate_model function."""

# Evaluate SVM models with different kernel functions
print("\nLinear Kernel:")
evaluate_model(svm_linear, X_test, y_test)

"""The result of the evaluation for the SVM model with a linear kernel function is as follows:

* Accuracy: The accuracy of the model is 0.6125, indicating that approximately 61.25% of the predictions were correct.

* Classification Report:

  * The precision, recall, and F1-score are reported for each class (quality rating) in the dataset: 3, 4, 5, 6, 7, and 8.
  * Precision represents the proportion of true positive predictions out of all positive predictions made by the model.
  * Recall (also known as sensitivity) represents the proportion of true positive predictions out of all actual positive instances in the dataset.
  * F1-score is the harmonic mean of precision and recall, providing a balanced measure of model performance.
  * Support indicates the number of instances of each class in the test set.
* Confusion Matrix:

  * The confusion matrix summarizes the model's performance by comparing predicted labels with true labels.
  * Each row of the matrix represents the actual classes, while each column represents the predicted classes.
  * The diagonal elements represent the number of correctly classified instances for each class, while off-diagonal elements represent misclassifications.
  
Overall, the evaluation suggests that the model performs relatively well in predicting quality ratings 5 and 6, but it struggles with the extremes (ratings 3, 4, 7, and 8), as indicated by low precision, recall, and F1-score for those classes.

The below code snippet evaluates the SVM model with a polynomial kernel function using the previously defined evaluate_model function.
"""

print("\nPolynomial Kernel:")
evaluate_model(svm_poly, X_test, y_test)

"""The result of the evaluation for the SVM model with a polynomial kernel function is as follows:

* Accuracy: The accuracy of the model is 0.61875, indicating that approximately 61.875% of the predictions were correct.

* Classification Report:

  * The precision, recall, and F1-score are reported for each class (quality rating) in the dataset: 3, 4, 5, 6, 7, and 8.
  * Precision represents the proportion of true positive predictions out of all positive predictions made by the model.
  * Recall (also known as sensitivity) represents the proportion of true positive predictions out of all actual positive instances in the dataset.
  * F1-score is the harmonic mean of precision and recall, providing a balanced measure of model performance.
  * Support indicates the number of instances of each class in the test set.
* Confusion Matrix:

  * The confusion matrix summarizes the model's performance by comparing predicted labels with true labels.
  * Each row of the matrix represents the actual classes, while each column represents the predicted classes.
  * The diagonal elements represent the number of correctly classified instances for each class, while off-diagonal elements represent misclassifications.

Overall, the evaluation suggests that the model performs relatively well in predicting quality ratings 5 and 6, but it struggles with the extremes (ratings 3, 4, 7, and 8), as indicated by low precision, recall, and F1-score for those classes.

The below code snippet evaluates the SVM model with a radial basis function (RBF) kernel function using the previously defined evaluate_model function.
"""

print("\nRBF Kernel:")
evaluate_model(svm_rbf, X_test, y_test)

"""The result of the evaluation for the SVM model with a radial basis function (RBF) kernel function is as follows:

* Accuracy: The accuracy of the model is 0.65625, indicating that approximately 65.625% of the predictions were correct.

* Classification Report:

  * The precision, recall, and F1-score are reported for each class (quality rating) in the dataset: 3, 4, 5, 6, 7, and 8.
  * Precision represents the proportion of true positive predictions out of all positive predictions made by the model.
  * Recall (also known as sensitivity) represents the proportion of true positive predictions out of all actual positive instances in the dataset.
  * F1-score is the harmonic mean of precision and recall, providing a balanced measure of model performance.
  * Support indicates the number of instances of each class in the test set.
* Confusion Matrix:

  * The confusion matrix summarizes the model's performance by comparing predicted labels with true labels.
  * Each row of the matrix represents the actual classes, while each column represents the predicted classes.
  * The diagonal elements represent the number of correctly classified instances for each class, while off-diagonal elements represent misclassifications.
  
Overall, the evaluation suggests that the model performs relatively well in predicting quality ratings 5 and 6, but it struggles with the extremes (ratings 3, 4, 7, and 8), as indicated by low precision, recall, and F1-score for those classes.

The below function provides a convenient way to visualize the confusion matrix, helping to interpret the performance of a classification model.
"""

# Function to visualize confusion matrix
def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title(title)
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.show()

"""The below function provides a convenient way to visualize the classification report, allowing for easier interpretation of the precision, recall, and F1-score metrics across different classes in a classification problem."""

# Function to visualize classification report
def plot_classification_report(y_true, y_pred):
    clf_report = classification_report(y_true, y_pred, output_dict=True)
    plt.figure(figsize=(8, 6))
    sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap="viridis")
    plt.title("Classification Report")
    plt.show()

"""The below code snippet visualizes the results for the SVM model with a linear kernel function."""

# Visualize results for Linear Kernel
print("Linear Kernel:")
plot_confusion_matrix(y_test, svm_linear.predict(X_test), "Confusion Matrix - Linear Kernel")
plot_classification_report(y_test, svm_linear.predict(X_test))

"""**Here's a breakdown of the confusion matrix components in this image:**

* True Positive (TP):
  * The cells on the diagonal (from top left to bottom right) represent correct predictions where the predicted class matches the true class.
  * For instance, the model correctly predicted class 2 107 times and class 3 89 times.
* False Positive (FP) and False Negative (FN):
  * The non-diagonal cells represent incorrect predictions.
  * For instance, for the true class 2, there were 17 instances that were incorrectly classified as 3, and for the true class 3, there were 45 instances that were incorrectly classified as 2.
* It seems there are 5 classes in total (0 through 4), given the 5x5 matrix.
* The lighter the cell color, the fewer the instances in that category, with white representing zero instances. Darker cells have higher instances, and the scaling of color suggests the relative frequency of predictions for each cell.
* By reading this matrix:

  * Class 2 has the highest number of correct predictions (107).
  * Class 3 also has a high number of correct predictions (89).
  * Classes 0, 1, and 4 have relatively few instances, and the model did not predict any instances for class 0.
  * The model seems to confuse class 2 with class 3 and vice versa (45 instances of class 3 predicted as class 2, and 17 instances of class 2 predicted as class 3).

* The label at the top indicates that the matrix is the result of a model using a linear kernel, which suggests that the data was likely being separated by linear boundaries in the algorithm that produced this matrix.
* Overall, while the model performs well for certain classes, there seems to be room for improvement, particularly in reducing the misclassifications between classes 2 and 3.


**Here is a breakdown of the insights from the report:**

* Classes:
  * The y-axis shows four classes (labelled from 1 to 4).
  * These are the categories that the model is trying to predict.

* Metrics:
  * The x-axis lists three evaluation metrics: precision, recall, and F1-score.
  * Precision indicates the ratio of true positive predictions to the total predicted positives for each class.
  * Recall (also known as sensitivity) measures the ratio of true positive predictions to all actual positives.
  * F1-score is the harmonic mean of precision and recall and provides a single metric that balances both.
* Scores:

  * Class 1: Has high precision (0.6), but lower recall (0.58), resulting in an F1-score of 0.64.
  * Class 2: Demonstrates a balance between precision (0.8) and recall (0.66), with an F1-score of 0.62.
  * Class 3: Shows comparatively lower precision (0.4) and recall (0.6), with an F1-score of 0.74.
  * Class 4: The scores are not visible for precision and recall, but the F1-score is 0.
* Averages:
  * The report also includes macro and weighted averages for all the metrics across all classes.
  * Macro avg:
    * This represents the arithmetic mean of the scores for each class, treating all classes equally.
    * The macro average precision is 0.62, recall is 0.61, and F1-score is 0.23.
  * Weighted avg:
    * These averages account for the support (the number of true instances for each class).
    * This can give a better measure of the true quality of the classifier, especially when there's a class imbalance.
    * Weighted average precision is 0.49, recall is 0.61, and F1-score is 0.55.
* Insights:
  * The model is not performing uniformly across all classes, indicated by varying scores.
  * The F1-score for Class 4 is 0, which suggests that the model fails to correctly identify any instances of this class, or there are no instances of this class in the test set.
  * The high score for Class 2 precision suggests that when the model predicts Class 2, it is very likely to be correct.
  * The macro average F1-score seems to be disproportionately low compared to the precision and recall, which might be due to a very low F1-score for one or more classes (e.g., Class 4).
  * The weighted average scores are generally lower than the macro averages (except recall), indicating that classes with more instances (higher support) might perform worse than those with fewer instances.
  
In conclusion, the model shows variability in performance across different classes, and particular attention might be required to improve its predictive power, especially for Class 3 and Class 4.

The below code snippet visualizes the results for the SVM model with a polynomial kernel function.
"""

# Visualize results for Polynomial Kernel
print("Polynomial Kernel:")
plot_confusion_matrix(y_test, svm_poly.predict(X_test), "Confusion Matrix - Polynomial Kernel")
plot_classification_report(y_test, svm_poly.predict(X_test))

"""**Here are some insights from the matrix:**
* Size of the Matrix: The confusion matrix is a 5x5 grid, indicating that the model is classifying data into one of five categories, labeled from 0 to 4.

* Diagonal Values (True Positives):
  * The largest numbers in the matrix appear along the diagonal, from the top left to the bottom right.
  * These values represent the number of true positives, meaning the instances where the model correctly predicted the true label.
  * For example, it correctly predicted label '0' 108 times and label '3' 78 times.

* Off-Diagonal Values (Misclassifications):
  * The numbers not on the diagonal represent misclassifications.
  * For example, the model predicted label '1' 8 times when the true label was '0', and it predicted label '3' 16 times when the true label was '2'.

* Label 3 Predictions: It seems there is some confusion between labels '2' and '3', given the relatively high number (16) of instances where label '3' was predicted for observations that were actually label '2'.

* Overall Performance:
 * The darker shades on the diagonal suggest that the diagonal values (true positives) are higher than the off-diagonal values (false positives and false negatives), which generally indicates a well-performing model.
 * However, there are notable exceptions like the confusion between labels '2' and '3', and to a lesser extent between '2' and '4', where the model seems to struggle.

* Label Balancing: The matrix shows varying numbers of true labels, suggesting that the dataset might not be balanced. For example, there are more instances of labels '0' and '3' compared to '1', '2', and '4'.

In conclusion, the confusion matrix suggests that while the model is performing well in general, especially for labels '0' and '3', there is some room for improvement, particularly in differentiating between labels '2' and '3', and possibly achieving better balance in label distribution across the dataset.


**Here are the insights I can gather from the image:**

* Class Labels: There seem to be six classes (numbered from 0 to 5) for which the classification performance is measured.

* Precision:
  * Precision measures the accuracy of the positive predictions for each class (i.e., the number of true positives divided by the number of true positives and false positives).
  * From the image, we can see that classes 0, 1, and 2 have higher precision values (0.67, 0.63, and 0.62, respectively) compared to class 3, which has a notably lower precision of 0.29.

* Recall:
  * Recall (or sensitivity) measures how many of the actual positive cases were captured by the model (i.e., the number of true positives divided by the number of true positives and false negatives).
  * The recall values for classes 0 and 2 are identical (0.6), while class 1 has a slightly lower recall of 0.58. Class 3 has the highest recall of 0.87 among the individual classes.

* F1-Score:
  * The F1-score is the harmonic mean of precision and recall and is a measure of a test's accuracy.
  * It considers both the precision and the recall to compute the score.
  * Class 0 has the highest F1-score of 0.72, indicating a relatively balanced precision and recall, whereas class 3 has a lower F1-score of 0.4, which suggests a discrepancy between precision and recall for this class.

* Averages:

  * Macro Avg:
    * This is the unweighted average of the metric for each class.
    * The macro average values for precision, recall, and F1-score are low (0.32, 0.62, 0.29, respectively), which indicates there is significant variation in the performance across classes, with some classes performing much lower than others.
  * Weighted Avg:
    * This average takes into account the support for each class (the number of true instances for each class).
    * The weighted average is higher for precision (0.59), recall (0.62), and F1-score (0.59) compared to the macro average, suggesting that the classes with more instances might be performing better than those with fewer instances.
* Diagonal Values:
  * The diagonal values of the heatmap (from top left to bottom right) typically represent the recall for each class.
  * These are the values that ideally should be highest in each column for each class, as it shows how well the model is at correctly identifying each class.

* Color Coding:
  * The heat map uses color coding to represent metric values, with a key on the right-hand side.
  * Yellow represents higher values, purple represents lower values, and green is in between.
  * This visual representation quickly conveys which classes and metrics are performing well or poorly.

In summary, the classification model seems to perform variably across different classes. Some classes have a good balance of precision and recall, while at least one class (class 3) has a noticeable imbalance, with high recall but low precision, resulting in a lower F1-score. The macro average indicates that there may be classes which are significantly dragging down the overall performance of the classifier.

The below code snippet visualizes the results for the SVM model with a radial basis function (RBF) kernel function.
"""

# Visualize results for RBF Kernel
print("RBF Kernel:")
plot_confusion_matrix(y_test, svm_rbf.predict(X_test), "Confusion Matrix - RBF Kernel")
plot_classification_report(y_test, svm_rbf.predict(X_test))

"""** Here are the insights we can draw from this confusion matrix:**

* Classes in the Data: The matrix shows that there are six classes in total, labeled from 0 to 5. This is seen from both the x-axis (Predicted Labels) and the y-axis (True Labels).

* True Positives (Diagonal Elements): The diagonal elements, from the top left corner to the bottom right corner (8, 106, 89, 15), represent the number of correct predictions for each class, where the predicted label matches the true label.

* Off-Diagonal Elements:
  * The non-diagonal elements indicate the number of incorrect predictions.
  * For example:
    * The number 3 in the row for the true label 0 and the column of predicted label 1 signifies that the model incorrectly predicted class 1, three times when the true class was 0.
    * Similarly, the number 18 in the row for the true label 1 and the column of predicted label 2 shows that 18 instances of class 1 were wrongly predicted as class 2.
* Model Performance for Different Classes:
  * The performance seems to vary across different classes.
  * Class 1 has the highest number of true positives (106), indicating good model accuracy for this class.
  * However, Class 0 and Class 5 have significantly fewer correct predictions (8 and 15, respectively), suggesting the model struggles with classifying these classes correctly.

* Imbalances and Misclassifications:
  * There are visible imbalances in misclassification;
  * for example,
    * Class 3 seems to be commonly confused with Class 1 and Class 2 (38 and 20 instances, respectively).
    * This might imply that the feature space for classes 1, 2, and 3 has overlaps, causing misclassifications.

* Overall Accuracy: While the exact accuracy cannot be calculated without the total number of instances, you can tell the model is performing relatively well for certain classes (especially Class 1) and not as well for others.

**Here's an interpretation of HEATMAP:**

* Matrix Structure and Labels:
  * The matrix consists of several rows and columns with numerical values.
  * We can observe the labels on the y-axis that represent the classes (from 0 to 5), which likely correspond to the different classes that the model has attempted to predict.
  * On the x-axis, we have the performance metrics labeled as 'precision', 'recall', and 'f1-score'.
  * Additionally, there are rows labeled 'macro avg' and 'weighted avg', which indicate the macro and weighted averages of the performance metrics across all classes.

* Precision:
  * Precision is a measure that tells us what proportion of positive identifications was actually correct.
  * The values for precision of each class range from 0.62 to 0.66, except for class 3, which has a precision of 0.
  * This suggests that the model did not correctly predict any instance of class 3.

* Recall:
  * Recall, or sensitivity, measures what proportion of actual positives was identified correctly.
  * The recall values for class 0 to class 4 vary from 0.36 to 0.85, with class 4 having the highest recall.
  * Similar to precision, class 3 has a recall of 0, which implies that the model failed to identify any true instance of class 3.

* F1-Score:
  * The F1-score is a harmonic mean of precision and recall and is a measure of a test's accuracy.
  * The F1-scores for class 0 to class 4 range from 0.45 to 0.74, with class 0 scoring the lowest and class 2 the highest.
  * Again, class 3 has an F1-score of 0.

* Macro Average and Weighted Average:
  * The macro average computes the metric independently for each class and then takes the average (hence treating all classes equally), resulting in 0.62 for precision, 0.36 for recall, and 0.31 for the F1-score.
  * The weighted average takes into account the imbalance in the number of instances for each class, offering a performance measure that is influenced by the class distribution. It has values of 0.61 for precision, 0.66 for recall, and 0.63 for the F1-score.

* Insight and Implications:
  * From the heatmap, we can deduce that the model performs reasonably well for some classes but is notably ineffective for class 3 for unknown reasons (given the data we have).
  * The low macro average recall (0.36) suggests that, overall, the model has a significant number of false negatives across the classes.
  * It's also clear that there's an imbalance in the dataset, as indicated by the discrepancy between the macro and weighted averages.

* Visualization:
  * The heat map uses color-coding to visually represent the magnitude of the values, with lighter colors (yellow) indicating higher values, and darker colors (purple) indicating lower values.
  * The color gradient aids in quickly identifying which classes and metrics indicate stronger or weaker performance.

The below code snippet calculates the accuracy scores for each SVM model with different kernel functions.
"""

# Accuracy scores for each model
accuracy_scores = {
    'Linear Kernel': svm_linear.score(X_test, y_test),
    'Polynomial Kernel': svm_poly.score(X_test, y_test),
    'RBF Kernel': svm_rbf.score(X_test, y_test)
}

"""The below code snippet creates a bar plot to visualize the accuracy scores of SVM models with different kernel functions."""

# Define vibrant colors
colors = ['#FF6F61', '#6B5B95', '#88B04B']

# Plotting
plt.figure(figsize=(10, 6))
bars = plt.bar(accuracy_scores.keys(), accuracy_scores.values(), color=colors)

# Add labels to bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 3), va='bottom')

plt.xlabel('SVM Kernel Function')
plt.ylabel('Accuracy')
plt.title('Accuracy of SVM Models with Different Kernel Functions')
plt.ylim(0.0, 1.0)
plt.show()

"""From the chart, we can derive several insights:

* Accuracy Scores:
  * The chart quantifies the accuracy of each model by the height of the bar.
  * The Linear kernel has an accuracy of approximately 0.612, the Polynomial kernel has an accuracy of around 0.619, and the RBF kernel has an accuracy of about 0.656.

* Performance Comparison:
  * The SVM model with the RBF kernel outperforms the other two models with respect to the accuracy metric shown.
  * The Polynomial kernel has slightly better accuracy than the Linear kernel.
  * The difference in performance between the Linear and Polynomial kernels is very small, while the RBF kernel's performance is noticeably better.
* Choice of Kernel Function:
  * The choice of kernel function can have a significant impact on the performance of an SVM model.
  * The RBF kernel may be more suitable for the particular dataset or problem used in this comparison.
  * Since the RBF kernel has the highest accuracy among the three, it might be the preferred choice in this case, assuming accuracy is the primary metric for model selection.
"""