# -*- coding: utf-8 -*-
"""Lab04_2348409.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bju9aFs69ulL2gfPM0bQjr0mV-jZ6u78

**LAB - 04 Principal Component Analysis**
 * *Created by : Angeline A*
 * *Register No: 2348409*
 * *Edited on: 09/03/2024*

The application of PCA over the "Pima Indians Diabetes" dataset. The canonical representation of a dataset is the following.
It is a datamatrix *X belong to R^(p*n)*
 where:
 * *n* is the number of samples
 * *p* is the number of the features

On the rows there are samples while on the columns there are features:

* *xi* belongs to *R^p* each sample is a vector in a *p*-dimensional space

* *pj* belongs to *R^n* each sample is a vector in a *n*-dimensional space
"""

import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import sklearn
from sklearn import datasets

"""In the above code,
* import pandas as pd: This imports the pandas library, a powerful data manipulation and analysis tool for Python. It is commonly aliased as pd for convenience.

* import matplotlib: This imports the matplotlib library, which is a popular plotting library in Python.

* import matplotlib.pyplot as plt: This imports the pyplot module from matplotlib. Pyplot provides a MATLAB-like interface for creating plots and visualizations. It is commonly aliased as plt for convenience.

* import numpy as np: This imports the NumPy library, which is a fundamental package for scientific computing with Python. It provides support for arrays, matrices, and mathematical functions.

* import seaborn as sns: This imports the seaborn library, which is another Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.

* import sklearn: This imports the scikit-learn library, which is a machine learning library in Python. It provides simple and efficient tools for data mining and data analysis.

* from sklearn import datasets: This imports the datasets submodule from scikit-learn. This submodule provides various datasets that are commonly used for practicing and learning machine learning algorithms.
"""

dataframe = pd.read_csv("/content/diabetes.csv")
n = np.shape(dataframe)[0]
p = np.shape(dataframe)[1]
print("Shape of Dataset: {}\n\t* Number of samples:\t{}\n\t* Number of features:\t{}"
      .format(np.shape(dataframe), n, p))

dataframe.head()

"""This is a first inspection about
 * Type of columns
 * Missing values
"""

print(dataframe.isnull().count())

"""From the above code, we can observe that there are no null values in the datsets

These are some statistics useful to steer the whole analysis:
 * Descriptive statistics
 * Skewness
 * Kurtosis

**UNIVARIATE ANALYSIS**
"""

print(dataframe.describe(include='all'))

"""Here's a breakdown of the statistics:

* Pregnancies: The number of pregnancies ranges from 0 to 17, with a mean of approximately 3.85.
* Glucose: Glucose levels range from 0 to 199, with a mean of approximately 120.89.
* BloodPressure: Blood pressure ranges from 0 to 122, with a mean of approximately 69.11.
* SkinThickness: Skin thickness ranges from 0 to 99, with a mean of approximately 20.54.
* Insulin: Insulin levels range from 0 to 846, with a mean of approximately 79.80.
* BMI: BMI ranges from 0 to 67.1, with a mean of approximately 31.99.
DiabetesPedigreeFunction: This feature ranges from 0.078 to 2.42, with a mean of approximately 0.47.
* Age: Age ranges from 21 to 81, with a mean of approximately 33.24.
* Outcome: This is a binary feature indicating the presence (1) or absence (0) of diabetes. The mean value suggests that roughly 35% of individuals in the dataset have diabetes.
* Additionally, the "count" row indicates that there are 768 entries for each feature, suggesting that there are no missing values in the dataset. However, it's worth noting that some features have minimum values of 0 (such as Glucose, BloodPressure, SkinThickness, Insulin, BMI), which might indicate missing or invalid data that should be further investigated or handled appropriately.

The below code will show how the data are distributed

**PREGNANCIES**
"""

sns.histplot(dataframe['Pregnancies'], bins=10, kde=True)
plt.title('Histogram of Pregnancies')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""The above chart explains how the pregnacy data is being distributed which is not distributed normally.

**GLUCOSE**
"""

sns.histplot(dataframe['Glucose'], bins=10, kde=True)
plt.title('Histogram of Glusoce')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""From the above code we can infer that the Glucose level is almost normally distributed among the patients.

**BloodPressure**
"""

sns.histplot(dataframe['BloodPressure'], bins=10, kde=True)
plt.title('Histogram of BloodPressure')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""From the above graph, we can infer that the BloodPressure data is almost normally distributed.

**Insulin**
"""

sns.histplot(dataframe['Insulin'], bins=10, kde=True)
plt.title('Histogram of Insulin')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""From the above graph we can infer that the insulin level is normally distributed among the patients.

**AGE**
"""

sns.histplot(dataframe['Age'], bins=10, kde=True)
plt.title('Histogram of Age')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""From the above graph, we can infer that the age is normally distributed among the diabetic patients which means that people from age different age groups are affected by diabetes.

**BIVARIATE ANALYSIS**
"""

# @title Glucose vs Insulin

dataframe.plot(kind='scatter', x='Glucose', y='Insulin', s=40, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

"""From the above graph, we can infer that for some the patients when the glucose level increase the insulin level also increase but for some of the patients the insulin level remains the same even when the glucose level increases.

**Correlation between the numerical Values**
"""

dataframe.corr()

"""In most of the cases, there is weak correlation except for Age and pregancies which has correlation 5.44 and that too not strong

**Dropping the columns**

Checking for Skewness

The below code calculates the skewness of each column in the DataFrame after removing the 'Outcome' column. It helps in understanding the shape and distribution of the data in each feature.
"""

print(dataframe.drop(['Outcome'], axis=1).skew(axis=0))

"""The result is interpreted as,
* Pregnancies: Skewness of approximately 0.90, suggesting a moderately right-skewed distribution.

* Glucose: Skewness of approximately 0.17, indicating a slightly right-skewed distribution.

* BloodPressure: Skewness of approximately -1.84, suggesting a moderately left-skewed distribution.

* SkinThickness: Skewness of approximately 0.11, indicating a nearly symmetric distribution, slightly right-skewed.

* Insulin: Skewness of approximately 2.27, indicating a highly right-skewed distribution.

* BMI: Skewness of approximately -0.43, suggesting a slightly left-skewed distribution.

* DiabetesPedigreeFunction: Skewness of approximately 1.92, indicating a highly right-skewed distribution.

* Age: Skewness of approximately 1.13, suggesting a moderately right-skewed distribution.

Understanding the skewness of features helps in assessing their distributions. Positive skewness indicates a heavier right tail, while negative skewness indicates a heavier left tail. A skewness close to zero suggests a relatively symmetric distribution. Depending on the analysis, skewed distributions might need to be transformed for better model performance or interpretation.

The above code calculates the skewness of each column in the DataFrame after removing the 'Outcome' column. It helps in understanding the shape and distribution of the data in each feature.

In the below code, Kurtosis helps in assessing the shape and tails of the distribution. Positive kurtosis indicates a relatively peaked distribution with heavy tails compared to a normal distribution, while negative kurtosis indicates a flatter distribution with lighter tails. A kurtosis of 3 indicates a normal distribution.
"""

print("\n",dataframe.drop(['Outcome'], axis=1).kurtosis(axis=0))

"""The interpretation of the result,
* Pregnancies: Kurtosis value of approximately 0.16, suggesting a distribution slightly more peaked than a normal distribution.

* Glucose: Kurtosis value of approximately 0.64, indicating a distribution with a moderate level of peakedness compared to a normal distribution.

* BloodPressure: Kurtosis value of approximately 5.18, indicating a distribution with a significantly higher peak and heavier tails compared to a normal distribution. This suggests a leptokurtic distribution, which is more peaked and has heavier tails than a normal distribution.

* SkinThickness: Kurtosis value of approximately -0.52, suggesting a distribution with lighter tails compared to a normal distribution. This indicates a platykurtic distribution, which is flatter than a normal distribution.

* Insulin: Kurtosis value of approximately 7.21, indicating a distribution with a very high peak and heavier tails compared to a normal distribution. This suggests a leptokurtic distribution.

* BMI: Kurtosis value of approximately 3.29, indicating a distribution with a higher peak and heavier tails compared to a normal distribution. This suggests a leptokurtic distribution.

* DiabetesPedigreeFunction: Kurtosis value of approximately 5.59, indicating a distribution with a significantly higher peak and heavier tails compared to a normal distribution. This suggests a leptokurtic distribution.

* Age: Kurtosis value of approximately 0.64, indicating a distribution with a moderate level of peakedness compared to a normal distribution.

These kurtosis values provide insights into the shape and tails of the distributions of each feature, helping in understanding the data's characteristics and potential model assumptions.

This below code will generate a boxplot for each numerical feature in the DataFrame, providing a visual summary of their distribution, central tendency, and variability, which can be useful for identifying outliers and understanding the spread of the data.
"""

plt.figure(figsize=(10,6))
dataframe.drop(['Outcome'], axis=1).boxplot(figsize=(10,6))
plt.xticks(rotation=90)

"""Here are some specific observations about the data distribution based on the boxplot:

* Pregnancies: The median number of pregnancies is 0. Most people have not had any pregnancies. There are a few outliers with more than 7 pregnancies.
* Glucose: Blood glucose levels are spread out over a wide range. There are a few outliers with very high blood sugar levels.
* Blood pressure: Blood pressure levels are also spread out over a wide range, with a few outliers on both the high and low ends.
* Skin thickness: Skin thickness seems to be relatively normally distributed, with most people having a skin thickness between 0 and 25 millimeters. There are a few outliers on the high end.
* Insulin: Insulin levels are spread out over a wide range. There are a few outliers with very high insulin levels.
* BMI: Body mass index (BMI) seems to be somewhat normally distributed, with most people having a BMI between 20 and 40. There are a few outliers on both the high and low ends.
* Diabetes pedigree function: The diabetes pedigree function seems to be spread out over a wide range. It is difficult to make any conclusions about the distribution from this plot.
* Age: Age is spread out over a wide range. There are a few outliers who are older than 80 years old.

In the below code, we are dropping the column 'Outcome' which is not neccesarry for the reduction of the dimensionality.
"""

df_drop=dataframe.drop(labels=['Outcome'],axis=1)
df_drop.head()

X = df_drop.iloc[:,1:9].values
y = df_drop.iloc[:,0].values

"""After executing the above code, X will contain the feature values, and y will contain the corresponding target variable values. This format is suitable for many machine learning algorithms where X represents the input features and y represents the target variable to be predicted.





"""

X.shape

y.shape

from sklearn.preprocessing import StandardScaler
X_std = StandardScaler().fit_transform(X)

"""After executing the above code, X_std will contain the standardized feature matrix, where each feature has a mean of 0 and a standard deviation of 1. Standardization is commonly performed as a preprocessing step before applying machine learning algorithms, especially when features have different scales, to ensure that all features contribute equally to the analysis.

**CONVENTIONAL METHOD OF PCA**
* Computation of Covariance matrix
* Finding the Eigen Vectors and Eigen Values

The below code calculates the covariance matrix, which describes the relationship between different features in the dataset. It's an essential step in various statistical analyses and machine learning algorithms, such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA), as it provides insights into the linear relationships between features and their variances.
"""

mean_vec = np.mean(X_std, axis=0)
cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)
print('Covariance matrix \n%s' %cov_mat)

"""From the above result, we can interpret that,
* Diagonal Elements (Variances):
 * The diagonal elements of the covariance matrix represent the variances of individual features.
 * For example, the value at position (1,1) represents the variance of the first feature, the value at (2,2) represents the variance of the second feature, and so on.
 * Higher values indicate higher variability in the corresponding feature.

* Off-Diagonal Elements (Covariances):
 * The off-diagonal elements represent the covariance between pairs of features.
 * Covariance measures how much two variables change together.
 * Positive values indicate that the features tend to increase or decrease together, while negative values indicate that they change in opposite directions.
 * The closer the value is to zero, the weaker the linear relationship between the features.

The below code calculates the covariance matrix using NumPy's built-in function, providing an alternative method to compute the covariance matrix compared to the previous approach. The resulting covariance matrix will have the same interpretation as described earlier: diagonal elements represent variances, and off-diagonal elements represent covariances between pairs of features.
"""

print('NumPy covariance matrix: \n%s' %np.cov(X_std.T))

"""The below code generates a heatmap visualization to depict the correlation (or covariance) between different features in the dataset. The color intensity represents the strength and direction of the correlation: darker colors indicate stronger positive or negative correlations, while lighter colors indicate weaker correlations or near-zero covariance. The numerical values in each cell provide additional information about the covariance between features. Heatmaps are useful for visualizing relationships and identifying patterns in large datasets."""

plt.figure(figsize=(9,9))
sns.heatmap(cov_mat, vmax=1, square=True,annot=True,cmap='cubehelix')

plt.title('Correlation between different features')

"""The below code calculates the eigenvalues and eigenvectors of the covariance matrix, which are essential in various mathematical and statistical analyses, including Principal Component Analysis (PCA). Eigenvalues represent the amount of variance explained by each eigenvector (or principal component), while eigenvectors represent the directions of maximum variance in the data. These values are crucial for understanding the structure and variability of the dataset.





"""

eig_vals, eig_vecs = np.linalg.eig(cov_mat)

print('Eigenvectors \n%s' %eig_vecs)
print('\nEigenvalues \n%s' %eig_vals)

"""From the above result, we cdan interpret that,
* Eigenvectors: Each column in the matrix represents an eigenvector corresponding to the eigenvalues. Eigenvectors represent directions in the feature space along which the data varies the most. These vectors are unit vectors, and their lengths (or magnitudes) indicate the amount of variance explained by the corresponding eigenvalues. In other words, the eigenvectors provide the directions of maximum variance in the data.

* Eigenvalues: These are the scalar values that indicate the amount of variance explained by each eigenvector. Larger eigenvalues correspond to eigenvectors that capture more variance in the data. In the context of Principal Component Analysis (PCA), the eigenvalues represent the importance of each principal component in describing the variability in the dataset.

**SELECTION OF PRINCIAPL COMPONENTS**

* T In order to decide which eigenvector(s) can dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues:
* The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped

The below code prepares a sorted list of eigenvalue-eigenvector pairs, sorted based on the magnitude of the eigenvalues. This sorting is important for identifying the principal components (eigenvectors) that explain the most variance in the dataset, which is a crucial step in Principal Component Analysis (PCA) or similar dimensionality reduction techniques.
"""

# Make a list of (eigenvalue, eigenvector) tuples
eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]

# Sort the (eigenvalue, eigenvector) tuples from high to low
eig_pairs.sort(key=lambda x: x[0], reverse=True)

# Visually confirm that the list is correctly sorted by decreasing eigenvalues
print('Eigenvalues in descending order:')
for i in eig_pairs:
    print(i[0])

"""The interpretation of the result is given by,
* 2.085505665964391: This eigenvalue indicates the amount of variance explained by the first principal component. It is the largest eigenvalue, suggesting that the first principal component captures the most variability in the dataset.

* 1.3220813548933916: This eigenvalue represents the amount of variance explained by the second principal component. It is the second-largest eigenvalue, indicating that the second principal component captures a significant amount of variability in the data, albeit less than the first component.

* 1.0308120217676302: This eigenvalue corresponds to the third principal component and explains the variance captured by it.

* 0.8741721592089317: This eigenvalue represents the variance explained by the fourth principal component.

* 0.6981399702401155: This eigenvalue corresponds to the fifth principal component.

* 0.5928584216191443: This eigenvalue represents the variance explained by the sixth principal component.

* 0.40555687305998045: This eigenvalue corresponds to the seventh principal component.

**Explained Variance**
* After sorting the eigenpairs, the next question is "how many principal components are we going to choose for our new feature subspace?"
* A useful measure is the so-called "explained variance," which can be calculated from the eigenvalues.
* The explained variance tells us how much information (variance) can be attributed to each of the principal components.

The below code explains,
var_exp will contain the percentage of variance explained by each principal component in descending order of importance. This information is valuable for assessing how much information each principal component captures from the original dataset and for determining the optimal number of principal components to retain in dimensionality reduction techniques like Principal Component Analysis (PCA).
"""

tot = sum(eig_vals)
var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]

"""The below code generates a bar plot to visualize the explained variance ratios of each principal component, providing insights into the amount of variance captured by each component. This visualization helps in understanding the contribution of each principal component to the overall variability in the dataset.





"""

with plt.style.context('dark_background'):
    plt.figure(figsize=(6, 4))

    plt.bar(range(7), var_exp, alpha=0.5, align='center',
            label='individual explained variance')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal components')
    plt.legend(loc='best')
    plt.tight_layout()

"""* The plot above clearly shows that maximum variance (somewhere around 29%) can be explained by the first principal component alone.
* The second principal component has the explained variance as around 19%.
* The 3rd and 4th princiapl components provides varinace somwhere around 15% and 13%.So those cannot be ignored.
* But we can drop the last 3 components as it has less than 10% of the variance

* The construction of the projection matrix that will be used to transform the Human resouces analytics data onto the new feature subspace.

* **Suppose only 1st and 2nd principal component shares the maximum amount of information say around 90%.Hence we can drop other components.**

* Here, we are reducing the 7-dimensional feature space to a 2-dimensional feature subspace, by choosing the “top 2” eigenvectors with the highest eigenvalues to construct our d×k-dimensional eigenvector matrix W

The below code constructs a transformation matrix matrix_w that can be used to transform the original feature space into a new feature space defined by the principal components with the highest eigenvalues.
"""

matrix_w = np.hstack((eig_pairs[0][1].reshape(7,1),
                      eig_pairs[1][1].reshape(7,1)
                    ))
print('Matrix W:\n', matrix_w)

"""The above code can be interpreted as,
* Column 1: The first column of Matrix W contains the eigenvector corresponding to the principal component with the highest eigenvalue. It indicates the direction of maximum variance in the dataset.

* Column 2: The second column of Matrix W contains the eigenvector corresponding to the principal component with the second-highest eigenvalue. It represents the direction of the second-largest variance in the dataset, orthogonal to the first principal component.

**Projection onto the New Feature Space**
* In this last step we will use the 7×2-dimensional projection matrix W to transform our samples onto the new subspace via the equation Y=X×W

The below code computes the linear transformation of the standardized feature matrix X_std into a new feature space defined by the principal components extracted from the original dataset.
"""

Y = X_std.dot(matrix_w)
Y

Y.shape

"""We have reduced the 7 dimensional dataset to 2 dimensional dataset without losing any of the information that is necessary for the prediction of the Diabetes in a patient.

**PCA with scikit-learn**

The below code generates a plot that helps visualize the trade-off between the number of principal components retained and the amount of variance explained by these components. It assists in determining the optimal number of principal components to retain for dimensionality reduction while preserving a significant amount of variance in the dataset.
"""

from sklearn.decomposition import PCA
pca = PCA().fit(X_std)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,7,1)
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')

"""The above graph visualizes how much of the variance in a target variable is explained by a model as more features are added. Here's a breakdown of the information in the plot:

* X-axis: This axis represents the number of components, which likely refers to the number of features or variables included in the model. As more features are added, the model complexity increases.

* Y-axis: This axis represents the cumulative explained variance, ranging from 0 (no variance explained) to 1 (all variance explained).

* Line: The line shows how the explained variance increases as more features are added to the model. An ideal scenario would be a curve that increases steadily towards 1, indicating the model is effectively capturing the relevant factors with each added feature.

In conclusion we can say that, the 6 princiapl components ar ecaturin gmore variance and we can drop the 7th one which doesn't contribute to the variability(Spread of the data).

From the above graph, we can see that how the principal components are wide spread and which principal component captures more varinace

The below code applies PCA to reduce the dimensionality of the dataset to 2 principal components while preserving as much variance as possible.
"""

from sklearn.decomposition import PCA
sklearn_pca = PCA(n_components=6)
Y_sklearn = sklearn_pca.fit_transform(X_std)

print(Y_sklearn)

"""We can see the relation between the features from the baove result which means it describes how they are varying from the other feature."""

Y_sklearn.shape

"""**Conclusion**

Initially we had nine features, In that we dropped the Outcome column since that is not necessary as it is the result of the all the featues.
* We have reduced the Dimension from 7 to 2 based on the manual method. The first principal component is Insulin, The 2nd Pricipal Component is Glucose.
* We reduced the Dimesion from 7 to 6 based on the in-built method from sklearn.
"""

